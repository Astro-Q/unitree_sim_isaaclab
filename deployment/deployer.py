#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹éƒ¨ç½²æ¨¡å—
æ”¯æŒå°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°ä»¿çœŸç¯å¢ƒæˆ–çœŸå®æœºå™¨äºº
"""

import os
import torch
import onnx
import onnxruntime as ort
from pathlib import Path
from typing import Dict, Optional, Tuple
import numpy as np

class ModelDeployer:
    """æ¨¡å‹éƒ¨ç½²å™¨"""
    
    def __init__(self,
                 model_path: str,
                 deployment_target: str = 'simulation',
                 config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–éƒ¨ç½²å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            deployment_target: éƒ¨ç½²ç›®æ ‡ ('simulation' æˆ– 'real_robot')
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.model_path = Path(model_path)
        self.deployment_target = deployment_target
        self.config_path = config_path
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ ¹æ®éƒ¨ç½²ç›®æ ‡é€‰æ‹©éƒ¨ç½²æ–¹å¼
        if deployment_target == 'simulation':
            self._deploy_to_simulation()
        elif deployment_target == 'real_robot':
            self._deploy_to_real_robot()
        else:
            raise ValueError(f"æœªçŸ¥çš„éƒ¨ç½²ç›®æ ‡: {deployment_target}")
    
    def _deploy_to_simulation(self):
        """éƒ¨ç½²åˆ°ä»¿çœŸç¯å¢ƒ"""
        print("=" * 60)
        print("ğŸš€ éƒ¨ç½²æ¨¡å‹åˆ°ä»¿çœŸç¯å¢ƒ")
        print("=" * 60)
        
        # è½¬æ¢ä¸ºONNXæ ¼å¼ï¼ˆä¾¿äºåœ¨ä»¿çœŸä¸­ä½¿ç”¨ï¼‰
        onnx_path = self.model_path.parent / f"{self.model_path.stem}.onnx"
        self._convert_to_onnx(onnx_path)
        
        print(f"âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºONNXæ ¼å¼: {onnx_path}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("1. åœ¨ sim_main.py ä¸­ä½¿ç”¨ --model_path å‚æ•°æŒ‡å®šONNXæ¨¡å‹è·¯å¾„")
        print("2. è®¾ç½® --action_source policy ä»¥ä½¿ç”¨ç­–ç•¥æ¨¡å‹")
        print("\nç¤ºä¾‹å‘½ä»¤:")
        print(f"python sim_main.py --task <TASK_NAME> --model_path {onnx_path} --action_source policy")
    
    def _deploy_to_real_robot(self):
        """éƒ¨ç½²åˆ°çœŸå®æœºå™¨äºº"""
        print("=" * 60)
        print("ğŸ¤– éƒ¨ç½²æ¨¡å‹åˆ°çœŸå®æœºå™¨äºº")
        print("=" * 60)
        
        # è½¬æ¢ä¸ºONNXæ ¼å¼ï¼ˆçœŸå®æœºå™¨äººé€šå¸¸ä½¿ç”¨ONNX Runtimeï¼‰
        onnx_path = self.model_path.parent / f"{self.model_path.stem}.onnx"
        self._convert_to_onnx(onnx_path)
        
        # åˆ›å»ºéƒ¨ç½²åŒ…
        deploy_dir = self.model_path.parent / "deployment_package"
        deploy_dir.mkdir(exist_ok=True)
        
        # å¤åˆ¶æ¨¡å‹æ–‡ä»¶
        import shutil
        shutil.copy(onnx_path, deploy_dir / "model.onnx")
        
        # åˆ›å»ºæ¨ç†è„šæœ¬
        self._create_inference_script(deploy_dir)
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        self._create_deployment_config(deploy_dir)
        
        print(f"âœ… éƒ¨ç½²åŒ…å·²åˆ›å»º: {deploy_dir}")
        print("\néƒ¨ç½²åŒ…åŒ…å«:")
        print("  - model.onnx: ONNXæ¨¡å‹æ–‡ä»¶")
        print("  - inference.py: æ¨ç†è„šæœ¬")
        print("  - config.json: é…ç½®æ–‡ä»¶")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print(f"cd {deploy_dir}")
        print("python inference.py")
    
    def _convert_to_onnx(self, output_path: Path):
        """è½¬æ¢ä¸ºONNXæ ¼å¼"""
        print(f"æ­£åœ¨è½¬æ¢æ¨¡å‹ä¸ºONNXæ ¼å¼...")
        
        # åŠ è½½PyTorchæ¨¡å‹
        from training.trainer import GraspingPolicy
        
        model = GraspingPolicy(
            image_channels=3,
            image_size=(224, 224),
            joint_dim=29,
            action_dim=29,
            hidden_dim=256
        )
        model.load_state_dict(torch.load(self.model_path, map_location=self.device))
        model.eval()
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        dummy_image = torch.randn(1, 3, 224, 224).to(self.device)
        dummy_joint_pos = torch.randn(1, 29).to(self.device)
        dummy_joint_vel = torch.randn(1, 29).to(self.device)
        
        # å¯¼å‡ºONNXï¼ˆæ³¨æ„ï¼šç”±äºæ¨¡å‹æ¥å—å¤šä¸ªè¾“å…¥ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹ç»“æ„
        try:
            torch.onnx.export(
                model,
                (dummy_image, dummy_joint_pos, dummy_joint_vel),
                str(output_path),
                input_names=['image', 'joint_positions', 'joint_velocities'],
                output_names=['action'],
                dynamic_axes={
                    'image': {0: 'batch_size'},
                    'joint_positions': {0: 'batch_size'},
                    'joint_velocities': {0: 'batch_size'},
                    'action': {0: 'batch_size'}
                },
                opset_version=11
            )
            print(f"âœ… ONNXè½¬æ¢æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ONNXè½¬æ¢å¤±è´¥: {e}")
            print("æç¤º: å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹ç»“æ„ä»¥æ”¯æŒONNXå¯¼å‡º")
    
    def _create_inference_script(self, deploy_dir: Path):
        """åˆ›å»ºæ¨ç†è„šæœ¬"""
        script_content = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåœ¨çœŸå®æœºå™¨äººä¸Šè¿è¡Œè®­ç»ƒå¥½çš„æ¨¡å‹
"""

import numpy as np
import onnxruntime as ort
import cv2
from typing import Dict, List

class ModelInference:
    """æ¨¡å‹æ¨ç†å™¨"""
    
    def __init__(self, model_path: str):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: ONNXæ¨¡å‹è·¯å¾„
        """
        self.session = ort.InferenceSession(model_path)
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        é¢„å¤„ç†å›¾åƒ
        
        Args:
            image: è¾“å…¥å›¾åƒ (H, W, C)
        
        Returns:
            é¢„å¤„ç†åçš„å›¾åƒ (1, C, H, W)
        """
        # è°ƒæ•´å¤§å°
        image = cv2.resize(image, (224, 224))
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼
        image = image.transpose(2, 0, 1)
        
        # å½’ä¸€åŒ–
        image = image.astype(np.float32) / 255.0
        
        # æ·»åŠ batchç»´åº¦
        image = np.expand_dims(image, axis=0)
        
        return image
    
    def predict(self, image: np.ndarray, 
                joint_positions: np.ndarray,
                joint_velocities: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            image: è¾“å…¥å›¾åƒ
            joint_positions: å…³èŠ‚ä½ç½®
            joint_velocities: å…³èŠ‚é€Ÿåº¦
        
        Returns:
            é¢„æµ‹çš„åŠ¨ä½œ
        """
        # é¢„å¤„ç†
        image_processed = self.preprocess_image(image)
        joint_pos = np.expand_dims(joint_positions.astype(np.float32), axis=0)
        joint_vel = np.expand_dims(joint_velocities.astype(np.float32), axis=0)
        
        # æ¨ç†
        inputs = {
            self.input_names[0]: image_processed,
            self.input_names[1]: joint_pos,
            self.input_names[2]: joint_vel
        }
        
        outputs = self.session.run(self.output_names, inputs)
        
        return outputs[0][0]  # è¿”å›ç¬¬ä¸€ä¸ªbatchçš„ç»“æœ

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ¨ç†")
    parser.add_argument("--model_path", type=str, default="model.onnx",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--image_path", type=str, required=True,
                       help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--joint_positions", type=str, required=True,
                       help="å…³èŠ‚ä½ç½®ï¼ˆé€—å·åˆ†éš”ï¼‰")
    parser.add_argument("--joint_velocities", type=str, required=True,
                       help="å…³èŠ‚é€Ÿåº¦ï¼ˆé€—å·åˆ†éš”ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inference = ModelInference(args.model_path)
    
    # åŠ è½½è¾“å…¥
    image = cv2.imread(args.image_path)
    joint_pos = np.array([float(x) for x in args.joint_positions.split(',')])
    joint_vel = np.array([float(x) for x in args.joint_velocities.split(',')])
    
    # é¢„æµ‹
    action = inference.predict(image, joint_pos, joint_vel)
    
    print("é¢„æµ‹çš„åŠ¨ä½œ:")
    print(action)

if __name__ == "__main__":
    main()
'''
        
        script_path = deploy_dir / "inference.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # æ·»åŠ æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
    
    def _create_deployment_config(self, deploy_dir: Path):
        """åˆ›å»ºéƒ¨ç½²é…ç½®æ–‡ä»¶"""
        import json
        
        config = {
            "model_path": "model.onnx",
            "input_image_size": [224, 224],
            "joint_dim": 29,
            "action_dim": 29,
            "device": "cpu",  # çœŸå®æœºå™¨äººé€šå¸¸ä½¿ç”¨CPU
            "inference_fps": 30
        }
        
        config_path = deploy_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)
    
    def deploy(self):
        """æ‰§è¡Œéƒ¨ç½²"""
        print("éƒ¨ç½²å®Œæˆï¼")
        print(f"éƒ¨ç½²ç›®æ ‡: {self.deployment_target}")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
